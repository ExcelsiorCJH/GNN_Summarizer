{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Summarizer Using PL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from models import Summarizer\n",
    "from types_ import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SummaExperiment Class with PL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        \n",
    "        with open(path, 'r') as f:\n",
    "            self.data = [json.loads(line) for line in f]\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of data.\"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentences = self.data[idx]['doc'].split('\\n')\n",
    "        labels = self.data[idx]['labels'].split('\\n')\n",
    "        labels = [int(label) for label in labels]\n",
    "        \n",
    "        return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaExperiment(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 model: Summarizer,\n",
    "                 params: dict) -> None:\n",
    "        \n",
    "        super(SummaExperiment, self).__init__()\n",
    "        \n",
    "        self.model = model\n",
    "        self.params = params\n",
    "        # self.curr_device = None\n",
    "        \n",
    "        \n",
    "    # ---------------------\n",
    "    # TRAINING\n",
    "    # ---------------------\n",
    "    def forward(self, docs, offsets, labels) -> Tensor:\n",
    "        return self.model(docs, offsets, labels)\n",
    "    \n",
    "    def loss_function(self, logits, labels):\n",
    "        labels = torch.cat(\n",
    "            [torch.tensor(label, dtype=torch.float) for label in labels]\n",
    "        )\n",
    "        labels = labels.view(-1, logits.size()[1]).to(DEVICE)\n",
    "        logits = logits.view(-1, logits.size()[1])\n",
    "        \n",
    "        bce_loss = F.binary_cross_entropy_with_logits(logits, labels)\n",
    "        return bce_loss\n",
    "    \n",
    "    def accuracy(self, logits, labels):\n",
    "        \"\"\"Computes the accuracy for multiple binary predictions\"\"\"\n",
    "\n",
    "        labels = torch.cat(\n",
    "            [torch.tensor(label, dtype=torch.float) for label in labels]\n",
    "        )\n",
    "        labels = labels.view(-1, logits.size()[1]).to(DEVICE)\n",
    "        logits = logits.view(-1, logits.size()[1])\n",
    "\n",
    "        preds = torch.round(logits)\n",
    "        corrects = (preds == labels).sum().float()\n",
    "        acc = corrects / labels.numel()\n",
    "        return acc\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        docs, offsets, labels = batch\n",
    "        \n",
    "        logits = self.forward(docs, offsets, labels)\n",
    "        train_loss = self.loss_function(logits, labels)\n",
    "        \n",
    "        tqdm_dict = {'train_loss': train_loss}\n",
    "        output = OrderedDict({\n",
    "            'loss': train_loss,\n",
    "            'progress_bar': tqdm_dict,\n",
    "            'log': tqdm_dict\n",
    "        })\n",
    "        return output\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        docs, offsets, labels = batch\n",
    "        \n",
    "        logits = self.forward(docs, offsets, labels)\n",
    "        val_loss = self.loss_function(logits, labels)\n",
    "        \n",
    "        # acc\n",
    "        val_acc = self.accuracy(logits, labels)\n",
    "        \n",
    "        output = OrderedDict({\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc,\n",
    "        })\n",
    "        return output\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        \"\"\"\n",
    "        Called at the end of validation to aggregate outputs\n",
    "        :param outputs: list of individual outputs of each validation step\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        val_loss_mean = 0\n",
    "        val_acc_mean = 0\n",
    "        for output in outputs:\n",
    "            val_loss_mean += output['val_loss']\n",
    "            val_acc_mean += output['val_acc']\n",
    "        \n",
    "        val_loss_mean /= len(outputs)\n",
    "        val_acc_mean /= len(outputs)\n",
    "        tqdm_dict = {'val_loss': val_loss_mean, 'val_acc': val_acc_mean}\n",
    "        result = {'progress_bar': tqdm_dict, 'log': tqdm_dict, 'val_loss': val_loss_mean}\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        docs, offsets, labels = batch\n",
    "        logits = self.forward(docs, offsets, labels)\n",
    "        test_loss = self.loss_function(logits, labels)\n",
    "        \n",
    "        # acc\n",
    "        test_acc = self.accuracy(logits, labels)\n",
    "        \n",
    "        output = OrderedDict({\n",
    "            'test_loss': test_loss,\n",
    "            'test_acc': test_acc,\n",
    "        })\n",
    "        return output\n",
    "    \n",
    "    # ---------------------\n",
    "    # TRAINING SETUP\n",
    "    # ---------------------\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.model.parameters(),\n",
    "                               lr=self.params['LR'],\n",
    "                               weight_decay=self.params['weight_decay'])\n",
    "        \n",
    "        return [optimizer]\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def __collate_fn(batch):\n",
    "        docs = [entry[0] for entry in batch]\n",
    "        labels = [entry[1] for entry in batch]\n",
    "        offsets = [0] + [len(doc) for doc in docs]\n",
    "        return docs, offsets, labels\n",
    "    \n",
    "    def __dataloader(self, phase='train'):\n",
    "        \n",
    "        data_path = '../../data/summary/data/train.json'\n",
    "        dataset = SummaryDataset(data_path)\n",
    "        \n",
    "        # split - train/valid/test\n",
    "        train_size = int(0.6 * len(dataset))\n",
    "        valid_size = int(0.2 * len(dataset))\n",
    "        test_size = len(dataset) - (train_size + valid_size)\n",
    "\n",
    "        train_dataset, valid_dataset, test_dataset \\\n",
    "            = random_split(dataset, [train_size, valid_size, test_size])\n",
    "        \n",
    "        if phase == 'train':\n",
    "            loader =  DataLoader(train_dataset, \n",
    "                                 batch_size=1, \n",
    "                                 shuffle=True, \n",
    "                                 collate_fn=self.__collate_fn)\n",
    "        elif phase == 'valid':\n",
    "            loader =  DataLoader(valid_dataset, \n",
    "                                 batch_size=1, \n",
    "                                 shuffle=False, \n",
    "                                 collate_fn=self.__collate_fn)\n",
    "        elif phase == 'test':\n",
    "            loader =  DataLoader(test_dataset, \n",
    "                                 batch_size=1, \n",
    "                                 shuffle=False, \n",
    "                                 collate_fn=self.__collate_fn)\n",
    "        \n",
    "        return loader\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        # log.info('Training data loader called.')\n",
    "        print('Training data loader called.')\n",
    "        return self.__dataloader(phase='train')\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        # log.info('Validation data loader called.')\n",
    "        print('Validation data loader called.')\n",
    "        return self.__dataloader(phase='valid')\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        # log.info('Test data loader called.')\n",
    "        print('Test data loader called.')\n",
    "        return self.__dataloader(phase='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Summary Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "    docs = [entry[0] for entry in batch]\n",
    "    labels = [entry[1] for entry in batch]\n",
    "    \n",
    "    offsets = [0] + [len(doc) for doc in docs]\n",
    "        \n",
    "    return docs, offsets, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummaryDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        \n",
    "        with open(path, 'r') as f:\n",
    "            self.data = [json.loads(line) for line in f]\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of data.\"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentences = self.data[idx]['doc'].split('\\n')\n",
    "        labels = self.data[idx]['labels'].split('\\n')\n",
    "        labels = [int(label) for label in labels]\n",
    "        \n",
    "        return sentences, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'LR': 0.001,\n",
    "    'weight_decay': 0\n",
    "}\n",
    "\n",
    "model = Summarizer(in_dim=768,\n",
    "                   hidden_dim=128,\n",
    "                   out_dim=64,\n",
    "                   num_heads=2,\n",
    "                   num_classes=1).to(DEVICE) \n",
    "experiment = SummaExperiment(model, hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation data loader called.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Validation sanity check', layout=Layout(flex='2'), max=5.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaab69d7dc9e4771acfd23234648bf56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max=1.0), HTML(value='')), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data loader called.\n",
      "Validation data loader called.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
