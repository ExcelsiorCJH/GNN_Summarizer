{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPM using Kowiki Dataset using Huggingface's Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. Dataset Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `glob` 모듈을 이용해 kowiki dataset을 로드한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_lists = glob('../data/kowiki/*/*')\n",
    "file_lists.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Data Preprocessing for SPM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SentencePiece Model의 Input에 맞게끔 데이터를 처리해준다.\n",
    "\n",
    "- `nltk` 모듈을 이용해 문서를 문장 단위로 분리한다.\n",
    "\n",
    "- `spm` 모듈의 input을 위해 `spm_input.txt`를 생성한다.\n",
    "\n",
    "- reformer를 pretraining 시키기 위한 corpus를 pkl로 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d83088a95dec4fa0b5a35e9f0112b11a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=676.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for file_path in tqdm(file_lists):\n",
    "    items = []\n",
    "    with open(file_path, encoding=\"utf-8\") as source:\n",
    "        raw_text = source.readlines()\n",
    "        for obj in raw_text:\n",
    "            text = json.loads(obj)['text']\n",
    "            text = re.sub('\\\\n', ' ', text)\n",
    "            text = re.sub('\\\\s+', ' ', text)\n",
    "            items.append(text)\n",
    "    \n",
    "    for text in items:\n",
    "        sents = sent_tokenize(text)\n",
    "        corpus.extend(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "\n",
    "# save corpus for reformer pretraining\n",
    "with open('../data/corpus/kowiki_corpus.pkl', 'wb') as f:\n",
    "    dill.dump(corpus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = '../data/spm/spm_input.txt'\n",
    "\n",
    "with open(input_file, 'w', encoding='utf-8') as f:\n",
    "    for sent in corpus:\n",
    "        f.write(f'{sent}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. Train SentencePieceModel\n",
    "\n",
    "- HuggingFace's `tokenizers` 모듈을 이용하여 Tokenizer 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import SentencePieceBPETokenizer, BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a tokenizer\n",
    "# tokenizer = SentencePieceBPETokenizer()\n",
    "tokenizer = BertWordPieceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 52s, sys: 8.84 s, total: 7min 1s\n",
      "Wall time: 6min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# And then train\n",
    "tokenizer.train('../data/spm/spm_input.txt', show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/tokenizers/vocab.txt']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save tokenizers\n",
    "tokenizer.save('../data/tokenizers/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. Check SPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertWordPieceTokenizer(vocab_file='../data/tokenizers/vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=16, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing, original_str, normalized_str])\n"
     ]
    }
   ],
   "source": [
    "tmp_text = \"지미 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다.\"\n",
    "\n",
    "encoded = tokenizer.encode(tmp_text)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '지미',\n",
       " '카터',\n",
       " '##는',\n",
       " '조지아',\n",
       " '##주',\n",
       " '섬',\n",
       " '##터',\n",
       " '카운티',\n",
       " '플레이',\n",
       " '##ᆫ',\n",
       " '##스',\n",
       " '마을에서',\n",
       " '태어났다',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
