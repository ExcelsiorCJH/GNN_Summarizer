{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reformer Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00. Module Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import dill\n",
    "\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from barbar import Bar\n",
    "\n",
    "from transformers import BertTokenizer, PreTrainedTokenizer\n",
    "from fairseq.optim.adafactor import Adafactor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. Create WikiDataset\n",
    "\n",
    "- Reformer를 pretrain 하기 위해 SentDataset 클래스를 만들어 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, path, prefix=\"train\", is_sample=False):\n",
    "        \n",
    "        with open(path, 'rb') as f:\n",
    "            self.corpus = dill.load(f)\n",
    "            if is_sample:\n",
    "                self.corpus = self.corpus[:1000]\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of corpus.\"\"\"\n",
    "        return len(self.corpus)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.corpus[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test SentDataset \n",
    "corpus_path = '../data/corpus/kowiki_corpus.pkl'\n",
    "\n",
    "dataset = SentDataset(corpus_path, prefix='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len : 4353763\n",
      "수학의 기초에 대한 위기는 그 당시 수많은 논쟁에 의해 촉발되었으며, 그 논쟁에는 칸토어의 집합론과 브라우어-힐베르트 논쟁이 포함되었다.\n",
      "['지미 카터 제임스 얼 \"지미\" 카터 주니어(, 1924년 10월 1일 ~ )는 민주당 출신 미국 39번째 대통령 (1977년 ~ '\n",
      " '1981년)이다.',\n",
      " '지미 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다.',\n",
      " '조지아 공과대학교를 졸업하였다.']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "print('len :', len(dataset))\n",
    "print(dataset[100])\n",
    "pprint(dataset[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. ReformerTrainer Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BERT와 마찬가지로 Pretraining 단계에서 MLM(Masked-Language Model) Task를 이용해 학습시킴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERROR_LIST = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReformerTrainer(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 dataset,\n",
    "                 model,\n",
    "                 tokenizer,\n",
    "                 device=None,\n",
    "                 train_batch_size=8,\n",
    "                 eval_batch_size=None,\n",
    "                 tb_writer=True,\n",
    "                 tb_dir='./tb_logs',\n",
    "                 log_dir='./logs'):\n",
    "        \"\"\"\n",
    "        Provides an easy to use class for pretraining and evaluating a Reformer Model.\n",
    "        :param dataset: (torch.utils.data.Dataset) containing all of the data you wish to utilize during training.\n",
    "        :param model: (reformer_pytorch.Reformer)\n",
    "        :param tokenizer: (transformers.PreTrainedTokenizer) defaults to BertTokenizer ('bert-base-case')\n",
    "        :param device: provide manual device placement. If None, will default to cuda:0 if available.\n",
    "        :param tb_writer: (bool) Whether to write to tensorboard or not.\n",
    "        :param tb_dir: (str) Where to write TB logs to.\n",
    "        :param log_dir: (str) Where to write generic logs to.\n",
    "        \"\"\"\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.n_gpu = torch.cuda.device_count() if torch.cuda.is_available() else 0\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.eval_batch_size = eval_batch_size\n",
    "        self.tb_writer = tb_writer\n",
    "        self.log_dir = log_dir\n",
    "\n",
    "        if tokenizer is None:\n",
    "            self.tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "        if device is None:\n",
    "            self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        if eval_batch_size is None:\n",
    "            self.eval_batch_size = train_batch_size\n",
    "\n",
    "        if tb_writer:\n",
    "            from torch.utils.tensorboard import SummaryWriter\n",
    "            self.writer = SummaryWriter(log_dir=tb_dir)\n",
    "\n",
    "        logging.basicConfig(filename=f'{log_dir}/{datetime.now().date()}.log', level=logging.INFO)\n",
    "\n",
    "    def build_dataloaders(self, train_test_split=0.1, train_shuffle=True, eval_shuffle=True):\n",
    "        \"\"\"\n",
    "        Builds the Training and Eval DataLoaders\n",
    "        :param train_test_split: The ratio split of test to train data.\n",
    "        :param train_shuffle: (bool) True if you wish to shuffle the train_dataset.\n",
    "        :param eval_shuffle: (bool) True if you wish to shuffle the eval_dataset.\n",
    "        :return: train dataloader and evaluation dataloader.\n",
    "        \"\"\"\n",
    "        dataset_len = len(self.dataset)\n",
    "        eval_len = int(dataset_len * train_test_split)\n",
    "        train_len = dataset_len - eval_len\n",
    "        train_dataset, eval_dataset = random_split(self.dataset, (train_len, eval_len))\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.train_batch_size, shuffle=train_shuffle)\n",
    "        eval_loader = DataLoader(eval_dataset, batch_size=self.eval_batch_size, shuffle=eval_shuffle)\n",
    "        logging.info(f'''train_dataloader size: {len(train_loader.dataset)} | shuffle: {train_shuffle}\n",
    "                         eval_dataloader size: {len(eval_loader.dataset)} | shuffle: {eval_shuffle}''')\n",
    "        return train_loader, eval_loader\n",
    "\n",
    "    def mask_tokens(self, inputs: torch.Tensor, mlm_probability=0.15, pad=True):\n",
    "        \"\"\" Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. \"\"\"\n",
    "        labels = inputs.clone()\n",
    "        # mlm_probability defaults to 0.15 in Bert\n",
    "        probability_matrix = torch.full(labels.shape, mlm_probability)\n",
    "        special_tokens_mask = [\n",
    "            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "        ]\n",
    "        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "        if self.tokenizer._pad_token is not None:\n",
    "            padding_mask = labels.eq(self.tokenizer.pad_token_id)\n",
    "            probability_matrix.masked_fill_(padding_mask, value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
    "        inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "        if pad:\n",
    "            input_pads = self.tokenizer.max_len - inputs.shape[-1]\n",
    "            label_pads = self.tokenizer.max_len - labels.shape[-1]\n",
    "\n",
    "            inputs = F.pad(inputs, pad=(0, input_pads), value=self.tokenizer.pad_token_id)\n",
    "            labels = F.pad(labels, pad=(0, label_pads), value=self.tokenizer.pad_token_id)\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels\n",
    "\n",
    "    def _tokenize_input_ids(self, input_ids: list, pad_to_max_length: bool = True):\n",
    "        \"\"\"\n",
    "        Helper function to clean up the train and eval functions\n",
    "        :param input_ids: inputs to tokenize.\n",
    "        :param pad_to_max_length: Whether you want to pad the inputs to the tokenizer.max_len\n",
    "        :return: Tensor containing training data.\n",
    "        \"\"\"\n",
    "        inputs = torch.cat(\n",
    "            [\n",
    "                self.tokenizer.encode(\n",
    "                    input_ids[i],\n",
    "                    add_special_tokens=True,\n",
    "                    max_length=self.tokenizer.max_len,\n",
    "                    pad_to_max_length=pad_to_max_length,\n",
    "                    return_tensors='pt'\n",
    "                ) \\\n",
    "                for i in range(len(input_ids))\n",
    "            ]\n",
    "        )\n",
    "        return inputs\n",
    "\n",
    "    def train(self,\n",
    "              epochs,\n",
    "              train_dataloader,\n",
    "              eval_dataloader,\n",
    "              log_steps,\n",
    "              ckpt_steps,\n",
    "              ckpt_dir=None,\n",
    "              gradient_accumulation_steps=1):\n",
    "        \"\"\"\n",
    "        Trains the Reformer Model\n",
    "        :param epochs: The number of times you wish to loop through the dataset.\n",
    "        :param train_dataloader: (torch.utils.data.DataLoader) The data to train on.\n",
    "        :param eval_dataloader: (torch.utils.data.DataLoader) The data to evaluate on.\n",
    "        :param log_steps: The number of steps to iterate before logging.\n",
    "        :param ckpt_steps: The number of steps to iterate before checkpointing.\n",
    "        :param ckpt_dir: The directory to save the checkpoints to.\n",
    "        :param gradient_accumulation_steps: Optional gradient accumulation.\n",
    "        :return: Total number of steps, total loss, model\n",
    "        \"\"\"\n",
    "\n",
    "        optimizer = Adafactor(self.model.parameters())\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        losses = {}\n",
    "        global_steps = 0\n",
    "        local_steps = 0\n",
    "        step_loss = 0.0\n",
    "\n",
    "        if ckpt_dir is not None:\n",
    "            assert os.path.isdir(ckpt_dir)\n",
    "            try:\n",
    "                logging.info(f'{datetime.now()} | Continuing from checkpoint...')\n",
    "                self.model.load_state_dict(torch.load(f'{ckpt_dir}/model_state_dict.pt', map_location=self.device))\n",
    "                optimizer.load_state_dict(torch.load(f'{ckpt_dir}/optimizer_state_dict.pt'))\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.info(f'{datetime.now()} | No checkpoint was found | {e}')\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "        if self.n_gpu > 1:\n",
    "            self.model = nn.DataParallel(self.model)\n",
    "            logging.info(f'{datetime.now()} | Utilizing {self.n_gpu} GPUs')\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        logging.info(f'{datetime.now()} | Moved model to: {self.device}')\n",
    "        logging.info(\n",
    "            f'{datetime.now()} | train_batch_size: {self.train_batch_size} | eval_batch_size: {self.eval_batch_size}')\n",
    "        logging.info(f'{datetime.now()} | Epochs: {epochs} | log_steps: {log_steps} | ckpt_steps: {ckpt_steps}')\n",
    "        logging.info(f'{datetime.now()} | gradient_accumulation_steps: {gradient_accumulation_steps}')\n",
    "\n",
    "        for epoch in tqdm(range(epochs), desc='Epochs', position=0):\n",
    "            logging.info(f'{datetime.now()} | Epoch: {epoch}')\n",
    "            for step, batch in tqdm(enumerate(train_dataloader),\n",
    "                                    desc='Epoch Iterator',\n",
    "                                    position=1,\n",
    "                                    leave=True,\n",
    "                                    total=len(train_dataloader)):\n",
    "                for data in batch:\n",
    "                    inputs = self._tokenize_input_ids(data, pad_to_max_length=True)\n",
    "                    inputs, labels = self.mask_tokens(inputs)\n",
    "                    inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                    output = self.model(inputs)\n",
    "\n",
    "                    # only calculating loss on masked tokens\n",
    "                    loss_mx = labels != -100\n",
    "                    output = output[loss_mx].view(-1, self.tokenizer.vocab_size)\n",
    "                    labels = labels[loss_mx].view(-1)\n",
    "\n",
    "                    try:\n",
    "                        loss = loss_fn(output, labels)\n",
    "                    except:\n",
    "                        print('error 발생')\n",
    "                        print('inputs :', inputs)\n",
    "                        print('output :', output)\n",
    "                        print('labels :', labels)\n",
    "                        ERROR_LIST.append((inputs, output, labels))\n",
    "                        break\n",
    "\n",
    "                    if gradient_accumulation_steps > 1:\n",
    "                        loss /= gradient_accumulation_steps\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    self.model.zero_grad()\n",
    "\n",
    "                    step_loss += loss.item()\n",
    "                    losses[global_steps] = loss.item()\n",
    "                    local_steps += 1\n",
    "                    global_steps += 1\n",
    "\n",
    "                    if global_steps % log_steps == 0:\n",
    "                        if self.tb_writer:\n",
    "                            self.writer.add_scalar('Train/Loss', step_loss / local_steps, global_steps)\n",
    "                            self.writer.close()\n",
    "                        logging.info(\n",
    "                            f'''{datetime.now()} | Train Loss: {step_loss / local_steps} | Steps: {global_steps}''')\n",
    "\n",
    "                        with open(f'{self.log_dir}/train_results.json', 'w') as results_file:\n",
    "                            json.dump(losses, results_file)\n",
    "                            results_file.close()\n",
    "                        step_loss = 0.0\n",
    "                        local_steps = 0\n",
    "\n",
    "                    if global_steps % ckpt_steps == 0:\n",
    "                        # evaluating before every checkpoint\n",
    "                        self.evaluate(eval_dataloader)\n",
    "                        model_to_save = self.model.module if hasattr(self.model, 'module') else self.model\n",
    "                        torch.save(model_to_save.state_dict(), f'{ckpt_dir}/model_state_dict.pt')\n",
    "                        torch.save(optimizer.state_dict(), f'{ckpt_dir}/optimizer_state_dict.pt')\n",
    "\n",
    "                        logging.info(f'{datetime.now()} | Saved checkpoint to: {ckpt_dir}')\n",
    "\n",
    "        model_to_save = self.model.module if hasattr(self.model, 'module') else self.model\n",
    "        torch.save(model_to_save.state_dict(), f'{ckpt_dir}/model_state_dict.pt')\n",
    "        torch.save(optimizer.state_dict(), f'{ckpt_dir}/optimizer_state_dict.pt')\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def evaluate(self, dataloader):\n",
    "        \"\"\"\n",
    "        Runs through the provided dataloader with torch.no_grad()\n",
    "        :param dataloader: (torch.utils.data.DataLoader) Evaluation DataLoader\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        if self.n_gpu > 1 and not isinstance(self.model, nn.DataParallel):\n",
    "            self.model = nn.DataParallel(self.model)\n",
    "\n",
    "        self.model.eval()\n",
    "        eval_loss = 0.0\n",
    "        perplexity = 0.0\n",
    "        eval_steps = 0\n",
    "\n",
    "        logging.info(f'{datetime.now()} | Evaluating...')\n",
    "        for step, batch in tqdm(enumerate(dataloader), desc='Evaluating', leave=True, total=len(dataloader)):\n",
    "            for data in batch:\n",
    "                inputs = self._tokenize_input_ids(data, pad_to_max_length=True)\n",
    "                inputs, labels = self.mask_tokens(inputs)\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    output = self.model(inputs)\n",
    "\n",
    "                loss_mx = labels != -100\n",
    "                output_ids = output[loss_mx].view(-1, self.tokenizer.vocab_size)\n",
    "                labels = labels[loss_mx].view(-1)\n",
    "                try:\n",
    "                    tmp_eval_loss = loss_fn(output_ids, labels)\n",
    "                    tmp_perplexity = torch.exp(tmp_eval_loss)\n",
    "                except:\n",
    "                    print('evaluate')\n",
    "                    print('output :', output_ids)\n",
    "                    print('labels :', labels)\n",
    "                    ERROR_LIST.append((output_ids, labels))\n",
    "                    break\n",
    "\n",
    "                if self.n_gpu > 1:\n",
    "                    tmp_eval_loss = tmp_eval_loss.mean()\n",
    "\n",
    "                eval_loss += tmp_eval_loss.item()\n",
    "                perplexity += tmp_perplexity.item()\n",
    "                eval_steps += 1\n",
    "\n",
    "            eval_loss /= eval_steps\n",
    "            perplexity /= eval_steps\n",
    "\n",
    "            if self.tb_writer:\n",
    "                self.writer.add_scalar('Eval/Loss', eval_loss, eval_steps)\n",
    "                self.writer.close()\n",
    "                self.writer.add_scalar('Perplexity', perplexity, eval_steps)\n",
    "                self.writer.close()\n",
    "            logging.info(f'{datetime.now()} | Step: {step} | Eval Loss: {eval_loss} | Perplexity: {perplexity}')\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. Reformer Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "corpus_path = '../data/corpus/kowiki_corpus.pkl'\n",
    "dataset = SentDataset(corpus_path, prefix='train', is_sample=True)\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = BertTokenizer(vocab_file='../data/tokenizers/vocab.txt', max_len=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformer model\n",
    "model = ReformerLM(num_tokens=tokenizer.vocab_size, \n",
    "                   dim=512,\n",
    "                   depth=6,\n",
    "                   heads=8,\n",
    "                   max_seq_len=tokenizer.max_len,\n",
    "                   causal=True)\n",
    "\n",
    "# Reformer trainer\n",
    "trainer = ReformerTrainer(dataset, model, tokenizer, train_batch_size=2, eval_batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "train_dataloader, eval_dataloader = trainer.build_dataloaders(train_test_split=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d871cfd3f48345f582debb26bd6436a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epochs', max=3.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0813867dcba141bf9a7b61aa381b2789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch Iterator', max=50.0, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 발생\n",
      "inputs : tensor([[   2,   21,    3,  ...,    0,    0,    0],\n",
      "        [   2,   29,    3,  ...,    0,    0,    0],\n",
      "        [   2,   29,    3,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [   2, 1405, 1047,  ...,    0,    0,    0],\n",
      "        [   2, 1305,    3,  ...,    0,    0,    0],\n",
      "        [   2,   18,    3,  ...,    0,    0,    0]], device='cuda:0')\n",
      "output : tensor([], device='cuda:0', size=(0, 30000), grad_fn=<ViewBackward>)\n",
      "labels : tensor([], device='cuda:0', dtype=torch.int64)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.82 GiB (GPU 0; 10.92 GiB total capacity; 5.58 GiB already allocated; 495.81 MiB free; 9.66 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-33eeb3facc6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                       \u001b[0mckpt_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                       \u001b[0mckpt_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./ckpts'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                       gradient_accumulation_steps=1)\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./ckpts/model.bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-20cb5f3b2327>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, train_dataloader, eval_dataloader, log_steps, ckpt_steps, ckpt_dir, gradient_accumulation_steps)\u001b[0m\n\u001b[1;32m    181\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m                     \u001b[0;31m# only calculating loss on masked tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.6/site-packages/reformer_pytorch/reformer_pytorch.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_model_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.82 GiB (GPU 0; 10.92 GiB total capacity; 5.58 GiB already allocated; 495.81 MiB free; 9.66 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# train the Reformer\n",
    "model = trainer.train(epochs=3,\n",
    "                      train_dataloader=train_dataloader,\n",
    "                      eval_dataloader=eval_dataloader,\n",
    "                      log_steps=10,\n",
    "                      ckpt_steps=100,\n",
    "                      ckpt_dir='./ckpts',\n",
    "                      gradient_accumulation_steps=1)\n",
    "torch.save(model, './ckpts/model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
