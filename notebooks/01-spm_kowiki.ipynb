{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPM using Kowiki Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. Dataset Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `glob` 모듈을 이용해 kowiki dataset을 로드한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_lists = glob('../data/kowiki/*/*')\n",
    "file_lists.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Data Preprocessing for SPM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SentencePiece Model의 Input에 맞게끔 데이터를 처리해준다.\n",
    "\n",
    "- `nltk` 모듈을 이용해 문서를 문장 단위로 분리한다.\n",
    "\n",
    "- `spm` 모듈의 input을 위해 `spm_input.txt`를 생성한다.\n",
    "\n",
    "- reformer를 pretraining 시키기 위한 corpus를 pkl로 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d83088a95dec4fa0b5a35e9f0112b11a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=676.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for file_path in tqdm(file_lists):\n",
    "    items = []\n",
    "    with open(file_path, encoding=\"utf-8\") as source:\n",
    "        raw_text = source.readlines()\n",
    "        for obj in raw_text:\n",
    "            text = json.loads(obj)['text']\n",
    "            text = re.sub('\\\\n', ' ', text)\n",
    "            text = re.sub('\\\\s+', ' ', text)\n",
    "            items.append(text)\n",
    "    \n",
    "    for text in items:\n",
    "        sents = sent_tokenize(text)\n",
    "        corpus.extend(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "\n",
    "# save corpus for reformer pretraining\n",
    "with open('../data/corpus/kowiki_corpus.pkl', 'wb') as f:\n",
    "    dill.dump(corpus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = '../data/spm/spm_input.txt'\n",
    "\n",
    "with open(input_file, 'w', encoding='utf-8') as f:\n",
    "    for sent in corpus:\n",
    "        f.write(f'{sent}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03. Train SentencePieceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21min 57s, sys: 4.98 s, total: 22min 2s\n",
      "Wall time: 5min 33s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "vocab_size = 40000\n",
    "prefix = '200221_kowiki'\n",
    "cmd = f'--input={input_file} --model_prefix={prefix} --vocab_size={vocab_size}'\n",
    "\n",
    "spm.SentencePieceTrainer.Train(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04. Check SPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spm_path = '../data/spm/200221_kowiki.model'\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(f'{spm_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁지미', '▁카터', '는', '▁조지아주', '▁섬', '터', '▁카운티', '▁플레', '인', '스', '▁마을에서', '▁태어났다', '.']\n"
     ]
    }
   ],
   "source": [
    "tmp_text = \"지미 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다.\"\n",
    "\n",
    "s = sp.EncodeAsPieces(tmp_text)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
